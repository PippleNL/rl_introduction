{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UX-AC_UI7OQC"
   },
   "source": [
    "# Reinforcement Learning Introduction\n",
    "## RL applied to a simple use case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 1579,
     "status": "ok",
     "timestamp": 1675772576723,
     "user": {
      "displayName": "Lennart van Ham",
      "userId": "00077851678753008678"
     },
     "user_tz": -60
    },
    "id": "7Q3vOgGEB-Ke"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "# make sure that all columns of a pandas dataframe are printed\n",
    "pd.options.display.max_columns = None\n",
    "from itertools import *\n",
    "# for debugging\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gfugN70FyuSi"
   },
   "source": [
    "## Parameters\n",
    "It is inherent to Reinforcement Learning algorithms that there are many (hyper)parameters. Some are internal to the model (parameters), such as the state and action sizes. Others are explicitly specified parameters that control the training process (hyperparameters). It is important to tune these hyperparameters well because they have a large impact on the quality of the solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1675772579579,
     "user": {
      "displayName": "Lennart van Ham",
      "userId": "00077851678753008678"
     },
     "user_tz": -60
    },
    "id": "IXbsk0OhCWkq"
   },
   "outputs": [],
   "source": [
    "class Parameters():\n",
    "    \"\"\"\n",
    "    Define the necessary parameters\n",
    "\n",
    "    Attributes:\n",
    "        action_size: the size of the actions\n",
    "        state_size: the size of the states\n",
    "        index_state: the index of the state in the observations in the memory\n",
    "        index_action: the index of the action in the observations in the memory\n",
    "        index_reward: the index of the reward in the observations in the memory\n",
    "        index_next_state: the index of the next state in the observations in the memory\n",
    "        episodes_before_fit: the number of observations the agents gathers before fitting\n",
    "        steps_evaluation: the number of observations the agent gathers during an evaluation run\n",
    "        num_simulation_runs: the number of times the agent performs the iterate, train, evaluate cycle\n",
    "        expl_expl_method: the exploration exploitation method\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        # TODO replace all values with the correct use case values\n",
    "\n",
    "        # parameters\n",
    "        self.action_size = random.randint(5, 5)\n",
    "        self.state_size = random.randint(5, 5)\n",
    "        # TODO change according to use case\n",
    "        self.all_possible_actions = np.array(list((product([0, 1], repeat=self.state_size))))\n",
    "\n",
    "        self.index_state = 0\n",
    "        self.index_action = self.state_size\n",
    "        self.index_reward = self.index_action + self.action_size\n",
    "        self.index_next_state = self.index_reward + 1\n",
    "\n",
    "        # hyperparameters\n",
    "        self.episodes_before_fit = random.randint(3, 3)\n",
    "        self.max_step_per_episode = 1000\n",
    "        self.episodes_evaluation = 1\n",
    "        self.num_simulation_runs = random.randint(100, 200)\n",
    "        self.expl_expl_method = 'random'\n",
    "\n",
    "        print('Parameter class initialized...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tjqaTENF1OUM"
   },
   "source": [
    "## Environment\n",
    "![agent-environment interaction.png](data:image/png;base64,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)\n",
    "\n",
    "As can be seen in the figure, it is the task of the environment to provide the (next) state and reward, based on the action the agent chooses. Therefore, the environment class has as an attribute the initial state to provide a starting point. Furthermore, it has 3 functions:\n",
    "\n",
    "1.   **Get_next_state(self, state, action)**: this function provides, based on the current state and the action the agent chooses, the next state.\n",
    "\n",
    "2.   **Get_action_range(self, state)**: this function signals to the agent what all the possible actions are it can choose from based on the state it is currently in. It is possible that this function always returns the same, irrespective of the state.\n",
    "     \n",
    "     _For example, in the robot example, the possible actions are always up, right, down, left no matter where the robot is located in the maze. The reward is -1 when the robot walks into a wall. Alternatively, it is possible to prevent the agent from choosing an action which results in the robot walking into a wall._\n",
    "     \n",
    "     This is a design choice. Taking out 'infeasible' options helps the agent, since actions which are inherently bad are not even considered, so the agent does not even have to learn that the action is bad. However, this would introduce a variable number of available actions per state, which causes some RL methods to be unapplicable. It is especially useful to prevent the choice of infeasible actions in complex environments, where there are many state-action combinations, to reduce how much an agent has to learn about its environment.\n",
    "\n",
    "3. **Determine_reward(self, state, action)**: this function returns the reward based on the state and chosen action. This is one of the most important functions of the algorithm, if not the most important one, because it determines what the agent will learn. The reward should not be too specific, such that it dictates what the agent should choose, because the agent has to figure this out by itself. On the other hand, it should be possible for the agent to find a pattern in the state and actions which lead to good and bad results. In any case, all the information the reward is based on should be included in the state and action definition. \\\n",
    "   As mentioned under 2., a reward function may be used to filter out 'infeasible' options, by assigning (extremely) negative rewards.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 889,
     "status": "ok",
     "timestamp": 1675773289340,
     "user": {
      "displayName": "Lennart van Ham",
      "userId": "00077851678753008678"
     },
     "user_tz": -60
    },
    "id": "38Y3s_Yc7wl1"
   },
   "outputs": [],
   "source": [
    "class Environment():\n",
    "    \"\"\"\n",
    "    Class representing the environment\n",
    "\n",
    "    Attributes:\n",
    "        initial_state: the starting state of an iteration\n",
    "    \"\"\"\n",
    "    def __init__(self, par: Parameters):\n",
    "        # TODO replace value with actual value\n",
    "        self.initial_state = np.zeros(par.state_size)\n",
    "        self.terminal_state = np.ones(par.state_size)\n",
    "        self.all_possible_actions = par.all_possible_actions\n",
    "        print('Environment class initialized...')\n",
    "\n",
    "    def get_next_state(self, state: np.ndarray, action: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Determines the next state based on the current state and the performed action\n",
    "\n",
    "        Args:\n",
    "          state: the current state the agent is in\n",
    "          action: the action performed by the agent\n",
    "\n",
    "        Returns:\n",
    "          next_state: the next state the agent will be in\n",
    "        \"\"\"\n",
    "\n",
    "        # Set the next state equal to the selected action\n",
    "        next_state = action\n",
    "\n",
    "        # Return the next state\n",
    "        return next_state\n",
    "\n",
    "    def get_action_range(self, state: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Determines all possible actions the agent can choose in a certain state\n",
    "\n",
    "        Args:\n",
    "          state: the current state the agent is in\n",
    "\n",
    "        Returns:\n",
    "          all possible actions the agent can choose in the current state\n",
    "        \"\"\"\n",
    "\n",
    "        # Find all actions whose sum is larger than the current state\n",
    "        currently_possible = np.sum(self.all_possible_actions, axis=1) > sum(state)\n",
    "\n",
    "        # Return selection of full action range\n",
    "        return self.all_possible_actions[currently_possible]\n",
    "\n",
    "    def determine_reward(self, state: np.ndarray, action: np.ndarray) -> int:\n",
    "        \"\"\"\n",
    "        Determines the reward based on the state the agent is in and the action it chooses\n",
    "\n",
    "        Args:\n",
    "          state: the current state the agent is in\n",
    "          action: the action performed by the agent\n",
    "\n",
    "        Returns:\n",
    "          the reward corresponding to the current state and the chosen action\n",
    "        \"\"\"\n",
    "\n",
    "        # Return a reward of...\n",
    "        if sum(action) == sum(state) + 1:\n",
    "\n",
    "            # 1, if the sum of the selected action is 1 larger than the sum of the state\n",
    "            return 1\n",
    "        else:\n",
    "\n",
    "            # 0, otherwise\n",
    "            return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aKMjKzVG_LSM"
   },
   "source": [
    "## Q_table\n",
    "The ```Q_table``` class is initiated as an instance of the agent and contains its intelligence. That is, it has ```table``` and ```updated_table``` as attributes, which only differ from each other during training (the updating of the Q_table). This happens in the ```update_q_table(self, sample)``` function. This function loops over all the observations (state, action, reward, next state) gathered during an iteration. For every observation, there are 3 options:\n",
    "\n",
    "1.   **The state has not been encountered before** (the state is not yet a key of the Q_table dictionary): the state is added as a key in the Q_table dictionary, with a new dictionary as value. This dictionary has as its only key the current action and the value is the corresponding reward.\n",
    "2.   **The state has been encountered before, but the current action has not been chosen before in this state** (the state is a key of the Q_table dictionary, but the dictionary which is the value corresponding to this key does not contain the action yet as a key): the action is added as a key in the dictionary with the reward as corresponding value.\n",
    "3.   **The current state action combination has been selected before** (the dictionary corresponding to the state key in the Q_table already contains the current action as a key and corresponding Q_value): the value of the state-action combination is updated using the Bellmann equation:\n",
    "\n",
    "\\begin{align}\n",
    "Q(s,a) = r + \\max_{a'\\in A}Q(s', a')\n",
    "\\end{align}\n",
    "\n",
    "where $s'$, $a'$ indicate the next state and action, respectively, with action space $A$.\n",
    "\n",
    "Note that the updates are first only implemented in the ```updated_table```. This is the case because we need the original Q_table for predicting $\\max_{a'}Q(s', a')$. In this case, all the new Q_values are computed based on the current Q_value first and then the complete Q_table is updated.\n",
    "\n",
    "The Q_table class has an additional function ```get_best_action(self, state)```. This function simply returns the action corresponding to the highest Q_value for a certain state.\n",
    "\n",
    "Finally, the class has a function ```print_q_table(self)```, which provides a readable version of the Q-table as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "OEfL2KEhJKWz"
   },
   "outputs": [],
   "source": [
    "class Q_table():\n",
    "    \"\"\"\n",
    "    Class representing the Q_table\n",
    "\n",
    "    Attributes:\n",
    "        table: the current Q table\n",
    "        updated_table: the version of the Q table during updating (training)\n",
    "    \"\"\"\n",
    "    def __init__(self, par: Parameters):\n",
    "        self.table = {}\n",
    "        self.updated_table = {}\n",
    "        print('Q_table class initialized...')\n",
    "\n",
    "    def update_q_table(self, sample: np.ndarray) -> None:\n",
    "        \"\"\"\n",
    "        Updates/trains the q_table values\n",
    "\n",
    "        Args:\n",
    "            sample: the collection of observations (state, action, reward, next_state) gathered by the agent during the iteration\n",
    "        \"\"\"\n",
    "        # retrieve the state, action, reward, and next state from the sample\n",
    "        obs = [(tuple(obs[par.index_state:par.index_action]),\n",
    "                tuple(obs[par.index_action:par.index_reward]), \n",
    "                float(obs[par.index_reward:par.index_next_state]),\n",
    "                tuple(obs[par.index_next_state:]))\n",
    "               for obs in sample]\n",
    "\n",
    "        # For each observation...\n",
    "        for (state, action, reward, next_state) in obs:\n",
    "\n",
    "            # If the state is encountered for the first time, add it to the table\n",
    "            if state not in self.updated_table.keys(): # the state is not yet encountered\n",
    "                self.updated_table[state] = {action: reward}\n",
    "      \n",
    "            # if the state-action combination is not yet encountered (but the state has), add it to the table\n",
    "            elif action not in self.updated_table[state].keys():\n",
    "                self.updated_table[state][action] = reward\n",
    "      \n",
    "            # if the state-action pair has been encountered before, update the table\n",
    "            # value using the Bellmann equation\n",
    "            else:\n",
    "                if next_state in self.table.keys():\n",
    "                    future_exp_reward = self.table[next_state][self.get_best_action(next_state)]\n",
    "                else:\n",
    "                    future_exp_reward = 0\n",
    "                self.updated_table[state][action] = reward + future_exp_reward\n",
    "\n",
    "        # Overwrite the actual Q-table with the updated values\n",
    "        self.table = deepcopy(self.updated_table)\n",
    "\n",
    "    def get_best_action(self, state: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Gets the action which the agent thinks is best in the current state\n",
    "\n",
    "        Args:\n",
    "            state: the current state the agent is in\n",
    "\n",
    "        Returns:\n",
    "            the action with the highest q_value\n",
    "        \"\"\"\n",
    "        if state in self.table.keys():\n",
    "            # get the action corresponding to the highest q_value given the state\n",
    "            return max(self.table[state], key=self.table[state].get)\n",
    "        else:\n",
    "            print(f\"Warning state {state} is not yet in the Q_table, so a random action is returned instead of the best\")\n",
    "            return agent.choose_action(state=state, action_range=env.get_action_range(state=state), eval=False)\n",
    "\n",
    "    def print_q_table(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Prints the Q-table in a readable format\n",
    "        \n",
    "        Returns:\n",
    "            the q-table in data frame format\n",
    "        \"\"\"\n",
    "    \n",
    "        # convert the dictionary q-table to a data frame\n",
    "        df = pd.DataFrame.from_dict(self.table, orient='index')\n",
    "\n",
    "        # overwrite columns and indices by string for readability\n",
    "        df.columns = [str(column) for column in df.columns]\n",
    "        df.index = [str(index) for index in df.index]\n",
    "\n",
    "        # return the q-table as dataframe\n",
    "        return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TkK0fjvdFXwk"
   },
   "source": [
    "## Agent\n",
    "As can be seen in the figure above (in the Environment section), the agent receives the state and reward from the environment and returns an action. Hence, the two attributes of the agent class are the action it chooses and the Q_table class, which provides the intelligence of choosing an action. The only function this class has is ```choose_action(self, state, action_range, eval)```. Depending on whether the agent is evaluating, the agent chooses an action randomly from the action range (```eval = False```) or it chooses the action the agent thinks is best (```eval = True```). This function can be expanded to include more exploration-exploitation methods, e.g. Epsilon greedy or Boltzmann."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "ivw3_rSJ-E1S"
   },
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    \"\"\"\n",
    "    Class representing the agent\n",
    "\n",
    "    Attributes:\n",
    "        action: the action performed by the agent\n",
    "        q_table: instance of the Q_table class, which represents the intelligence of the agent\n",
    "    \"\"\"\n",
    "    def __init__(self, par: Parameters):\n",
    "        self.action = np.zeros([par.action_size])\n",
    "        self.q_table = Q_table(par)\n",
    "        print('Agent class initialized...')\n",
    "\n",
    "    def choose_action(self, state: np.ndarray, action_range: np.ndarray, eval: bool) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Chooses the next action based on the exploration exploitation method and whether it is an evaluation run or not\n",
    "\n",
    "        Args:\n",
    "            state: the current state the agent is in\n",
    "            action_range: all possible actions the agent can choose from the current state\n",
    "            eval: true when it is an evaluation run, otherwise false\n",
    "\n",
    "        Returns:\n",
    "            action: the chosen action\n",
    "        \"\"\"\n",
    "        if eval and tuple(state) in self.q_table.table.keys():\n",
    "            # choose the best action\n",
    "            action_tuple = self.q_table.get_best_action(tuple(state))\n",
    "            action = np.array(action_tuple)\n",
    "    \n",
    "        elif par.expl_expl_method == 'random':\n",
    "            # choose a random action from the action range\n",
    "            action = action_range[np.random.randint(0, action_range.shape[0])]\n",
    "            \n",
    "        else:\n",
    "            raise ValueError(\"The specified exploration exploitation method is not implemented, please specify a different method\")\n",
    "        \n",
    "        return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZwAlK7B1HL6l"
   },
   "source": [
    "## Running the simulation\n",
    "One simulation run always consists of 3 phases, which are executed in the ```iterate_train_evaluate(par, env, agent, eval_rewards)``` function:\n",
    "\n",
    "\n",
    "1.   **Iterate**: the iterate function goes through all the steps to gather observations to fill the memory. It takes the initial state as a starting point, then gets the range of possible actions in this state from the environment. Then, it feeds the state and action_range to the Agent to choose an action. Once the action is determined, the state and action are fed to the environment to obtain the reward and the next state. The state, action, reward, and next state are appended to the memory. Finally the current state is updated to the next state and the process starts again.\n",
    "2.   **Train**: Once all *num_steps* iterations are completed and the memory is filled, the train function is called. Since the implemented solver is a Q_table, training entails updating the Q_table.\n",
    "3.   **Evaluate**: After the Q_table is updated, it is time to evaluate how good the current Q_value estimations are. For this, we again perform several iterations, but this time the agent chooses the actions it thinks are best. The cumulative reward gathered during an evaluation run is used as a metric of how well the model is doing.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "1O8YPWf7NZX4"
   },
   "outputs": [],
   "source": [
    "def iterate_train_evaluate(par: Parameters, env: Environment, agent: Agent, eval_rewards: list) -> list:\n",
    "    \"\"\"\n",
    "    Performs the iterate, train, evaluate cycle once\n",
    "\n",
    "    Args:\n",
    "        par: instance of the Parameters class\n",
    "        env: instance of the Environment class\n",
    "        agent: instance of the Agent class\n",
    "        eval_rewards: list containing the cumulative evaluation rewards of every evaluation run\n",
    "\n",
    "    Returns:\n",
    "        eval_rewards: list containing the cumulative evaluation rewards of every evaluation run\n",
    "    \"\"\"\n",
    "    # perform the iterations to gain observations\n",
    "    memory = iterate(par.episodes_before_fit, eval=False)\n",
    "\n",
    "    # train the agent on the gathered observations\n",
    "    train(memory)\n",
    "\n",
    "    # evaluate the just trained agent\n",
    "    reward_eval_run = evaluate()\n",
    "\n",
    "    # save the evaluation rewards for plotting\n",
    "    eval_rewards.append(reward_eval_run)\n",
    "\n",
    "    return eval_rewards\n",
    "\n",
    "def iterate(num_episodes: int, eval: bool) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Gathers observations and thereby builds the memory of the agent\n",
    "\n",
    "    Args:\n",
    "        num_episodes: the number of observations which should be gathered during one iteration\n",
    "        eval: true when it is an evaluation run, otherwise false\n",
    "\n",
    "    Returns:\n",
    "        memory: the collection of observations (state, action, reward, next_state) gathered by the agent during the iteration\n",
    "    \"\"\"\n",
    "    # state, action, reward, next state are stored as observations in the memory\n",
    "    memory = []\n",
    "\n",
    "    # gather all the observations for this iteration\n",
    "    for episode in range(num_episodes):\n",
    "        state = env.initial_state\n",
    "        step = 0\n",
    "\n",
    "        # an episode is finished when either the terminal state is reached or the maximum allowed number of steps per episode are reached\n",
    "        while not all(state == env.terminal_state) and step < par.max_step_per_episode:\n",
    "      \n",
    "            # get the possible actions in the current state\n",
    "            action_range = env.get_action_range(state)\n",
    "\n",
    "            # choose an action\n",
    "            action = agent.choose_action(state, action_range, eval)\n",
    "\n",
    "            # determine the corresponding reward\n",
    "            reward = env.determine_reward(state, action)\n",
    "\n",
    "            # determine the next state\n",
    "            next_state = env.get_next_state(state, action)\n",
    "\n",
    "            # save the observation (state, action, reward, next state) in the memory\n",
    "            memory.append(np.concatenate([state, action, np.array([reward]), next_state]))\n",
    "\n",
    "            # set the current state to the next state and start the next iteration\n",
    "            state = next_state\n",
    "\n",
    "            step += 1\n",
    "\n",
    "    return np.array(memory)\n",
    "\n",
    "def train(memory: np.ndarray) -> None:\n",
    "    \"\"\"\n",
    "    Trains the model based on the memory\n",
    "\n",
    "    Args:\n",
    "        memory: the collection of observations (state, action, reward, next_state) gathered by the agent during the iteration\n",
    "    \"\"\"\n",
    "    # train by updating the q table values of the observations in the memory\n",
    "    agent.q_table.update_q_table(sample=memory)\n",
    "\n",
    "def evaluate() -> float:\n",
    "    \"\"\"\n",
    "    Evaluates the freshly trained model\n",
    "\n",
    "    Returns:\n",
    "        reward_eval_run: the cumulative reward obtained during the evaluation run\n",
    "    \"\"\"\n",
    "    # gather observations whilst letting the agent choose the action it thinks is best\n",
    "    memory = iterate(par.episodes_evaluation, eval=True)\n",
    "\n",
    "    # get the cumulative reward over the evaluation run\n",
    "    reward_eval_run = sum(memory[:,par.index_reward])\n",
    "\n",
    "    return reward_eval_run "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pG9M_C3nPFDr"
   },
   "source": [
    "## Plot the evaluation rewards\n",
    "The rewards are the most important evaluation method since it shows if the agent is learning. After that, whether the agent performs well on the problem KPIs is both dependent on the learning capabilities of the agent and the reward definition. If the reward is poorly defined, it could be the case that the agent can learn, but it simply does not learn the correct thing. When looking at the reward plot, the reward should at some point converge, irrespective of the relation between reward and KPIs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "1EsrBfYoZKce"
   },
   "outputs": [],
   "source": [
    "def plot(eval_rewards: list) -> None:\n",
    "    \"\"\"\n",
    "    Plot the rewards obtained during all evaluation runs\n",
    "\n",
    "    Args:\n",
    "    eval_rewards: list containing the cumulative evaluation rewards of every evaluation run\n",
    "    \"\"\"\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Reward')\n",
    "    plt.plot(eval_rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wBiKrhWvP7G9"
   },
   "source": [
    "## Main\n",
    "In the below piece of code the algorithm is started. First, it instantiates the Parameter, Environment, and Agent classes. Then, it executes ```num_simulation_runs``` (defined in parameters) times the iterate_train_evaluate process, upon which it plots the evaluation rewards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 591
    },
    "executionInfo": {
     "elapsed": 1280,
     "status": "ok",
     "timestamp": 1675238577880,
     "user": {
      "displayName": "Joyce Hendriks",
      "userId": "18195552816068485406"
     },
     "user_tz": -60
    },
    "id": "T__sY8YP-j1_",
    "outputId": "63a12f5f-1766-401c-f925-c942a2a786d1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter class initialized...\n",
      "Environment class initialized...\n",
      "Q_table class initialized...\n",
      "Agent class initialized...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The specified exploration exploitation method is not implemented, please specify a different method",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# perform num_simulation_runs times the iterate_train_evaluate cycle\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m run \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(par\u001b[38;5;241m.\u001b[39mnum_simulation_runs):\n\u001b[1;32m---> 11\u001b[0m     eval_rewards \u001b[38;5;241m=\u001b[39m \u001b[43miterate_train_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43magent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43magent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_rewards\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_rewards\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;66;03m# print upon run completion, for every tenfold and the final run\u001b[39;00m\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m run \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m run \u001b[38;5;241m==\u001b[39m par\u001b[38;5;241m.\u001b[39mnum_simulation_runs \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "Cell \u001b[1;32mIn[15], line 15\u001b[0m, in \u001b[0;36miterate_train_evaluate\u001b[1;34m(par, env, agent, eval_rewards)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03mPerforms the iterate, train, evaluate cycle once\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;124;03m    eval_rewards: list containing the cumulative evaluation rewards of every evaluation run\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# perform the iterations to gain observations\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m memory \u001b[38;5;241m=\u001b[39m \u001b[43miterate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpar\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepisodes_before_fit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43meval\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# train the agent on the gathered observations\u001b[39;00m\n\u001b[0;32m     18\u001b[0m train(memory)\n",
      "Cell \u001b[1;32mIn[15], line 54\u001b[0m, in \u001b[0;36miterate\u001b[1;34m(num_episodes, eval)\u001b[0m\n\u001b[0;32m     51\u001b[0m action_range \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mget_action_range(state)\n\u001b[0;32m     53\u001b[0m \u001b[38;5;66;03m# choose an action\u001b[39;00m\n\u001b[1;32m---> 54\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchoose_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction_range\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43meval\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# determine the corresponding reward\u001b[39;00m\n\u001b[0;32m     57\u001b[0m reward \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mdetermine_reward(state, action)\n",
      "Cell \u001b[1;32mIn[14], line 36\u001b[0m, in \u001b[0;36mAgent.choose_action\u001b[1;34m(self, state, action_range, eval)\u001b[0m\n\u001b[0;32m     33\u001b[0m     action \u001b[38;5;241m=\u001b[39m action_range[np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m, action_range\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])]\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 36\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe specified exploration exploitation method is not implemented, please specify a different method\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m action\n",
      "\u001b[1;31mValueError\u001b[0m: The specified exploration exploitation method is not implemented, please specify a different method"
     ]
    }
   ],
   "source": [
    "# create instances of all classes\n",
    "par = Parameters()\n",
    "env = Environment(par=par)\n",
    "agent = Agent(par=par)\n",
    "\n",
    "# initiate an empty list to which the evaluation rewards are appended\n",
    "eval_rewards = []\n",
    "\n",
    "# perform num_simulation_runs times the iterate_train_evaluate cycle\n",
    "for run in range(par.num_simulation_runs):\n",
    "    eval_rewards = iterate_train_evaluate(par=par, env=env, agent=agent, eval_rewards=eval_rewards)\n",
    "\n",
    "    # print upon run completion, for every tenfold and the final run\n",
    "    if run % 10 == 0 or run == par.num_simulation_runs -1:\n",
    "        print(f'Run {run}/{par.num_simulation_runs-1} completed')\n",
    "\n",
    "plot(eval_rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get some more insight in what the agent has actually learned, use the below code to print the generated Q-table. Remember, the row-axis gives all encountered states, the column axis all encountered actions. State-action combinations that are infeasible are printed with 'NaN'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 809
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1675238577881,
     "user": {
      "displayName": "Joyce Hendriks",
      "userId": "18195552816068485406"
     },
     "user_tz": -60
    },
    "id": "CKVPhKSCe1Hs",
    "outputId": "f36e9d13-a4ab-4fe7-a4aa-da12862c5352",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# print Q_table\n",
    "agent.q_table.print_q_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "1kKhigvKmZ1tw1-ghosg7JxaC4vWNWv72",
     "timestamp": 1675772377770
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
